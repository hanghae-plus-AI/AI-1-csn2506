{"metadata":{"colab":{"provenance":[{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/chapter3-1-huggingface-csn-7b3c58f7-8363-46ba-9be2-7f4450af6b8f.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20241008/auto/storage/goog4_request&X-Goog-Date=20241008T115103Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=a83d8a113d70fc5a879747d9629d1b38bbf93b43b5286f6d78a069e6f3049fee15dc666bfa6fad328a16a404235a195f1a144ce0748390082c7ae1fd11eb2bb8b691e2cfdfad51a7903082493b6fa775b4a367348ce60b8654d6eff0f6c3567b619b01ba0b2325c688382aab1696d74b22d335ffa471f4e4a9f77bd5507c834cecb5763bed9bd194449dcaa745a423defd99de355b616535c7dec801e3bff12ae2a79584a2cdc8572bfd339f0d52d5c46726751220459841ae8b255a76e44cd0df0a81fc36d8b44815d6f7ac60659c41240b47ddfe13238d965dd5180f40f2ed8d953effc9d02aaffe5c7964175d3020f2499bb4062fbf5a7886277067dda5fc","timestamp":1728388284077},{"file_id":"1iIhvEqckb-eDT5uhS92rnod2EVjSfj6K","timestamp":1728273345458}],"gpuType":"T4","machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["\n","# HuggingFace로 영화리뷰 감정 분석 모델 구현하기\n","\n","이번 과제에서는 이전 주차 과제에서 활용했던 `fancyzhx/ag_news` 문제를 HuggingFace로 구현하시면 됩니다. 다음 요구사항만 지키시면 됩니다.\n","\n","- `test` split은 학습에 활용되면 안됩니다.\n","- `trainer.train()`에 대한 log가 남아있어야 합니다.\n","- 최종 모델의 `test` split에 대한 정확도가 print되어야 하며, 90%를 넘기셔야 합니다.\n","- 다음 예시에 대한 예측 결과를 출력하셔야 합니다.\n","    \n","    ```\n","    UK charges 8 in terror plot linked to alert in US LONDON, AUGUST 17: Britain charged eight terror suspects on Tuesday with conspiracy to commit murder and said one had plans that could be used in striking US buildings that were the focus of security scares this month.\n","    ```\n","    \n","\n","이외에는 validation data 유무, 모델 architecture, hyper-parameter 등은 위의 조건만 만족한다는 가정 하에서 마음대로 수정하셔도 됩니다.\n","\n","필요한 library들을 설치하고 import합니다."],"metadata":{"id":"owTOpkIFX3wb"}},{"cell_type":"code","source":["!pip install transformers datasets evaluate accelerate scikit-learn"],"metadata":{"id":"UeHRwq3aR-bc","executionInfo":{"status":"ok","timestamp":1728273665675,"user_tz":-540,"elapsed":13554,"user":{"displayName":"조성남","userId":"10831514774218419222"}},"outputId":"1caae655-1656-4036-d46d-1312f644347d","execution":{"iopub.status.busy":"2024-10-08T11:29:55.036274Z","iopub.execute_input":"2024-10-08T11:29:55.036897Z","iopub.status.idle":"2024-10-08T11:30:08.503145Z","shell.execute_reply.started":"2024-10-08T11:29:55.036813Z","shell.execute_reply":"2024-10-08T11:30:08.502017Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.4.0)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\n","output_type":"stream"}]},{"cell_type":"code","source":["import random\n","import evaluate\n","import numpy as np\n","\n","from datasets import load_dataset\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification"],"metadata":{"id":"QObF2OAzYwLi","execution":{"iopub.status.busy":"2024-10-08T11:30:08.50558Z","iopub.execute_input":"2024-10-08T11:30:08.506265Z","iopub.status.idle":"2024-10-08T11:30:27.634558Z","shell.execute_reply.started":"2024-10-08T11:30:08.506215Z","shell.execute_reply":"2024-10-08T11:30:27.633612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dataset 준비\n","\n","그 다음 감정 분석을 위해 사용할 ag_news dataset을 `load_dataset` 함수로 다운로드 받습니다."],"metadata":{"id":"7vOSyAKGim9f"}},{"cell_type":"code","source":["ag_news = load_dataset(\"fancyzhx/ag_news\")# 데이터셋 변경\n","ag_news"],"metadata":{"id":"HzxfcmYPZA7x","executionInfo":{"status":"ok","timestamp":1728273717371,"user_tz":-540,"elapsed":5019,"user":{"displayName":"조성남","userId":"10831514774218419222"}},"outputId":"34df313e-4ec0-4db7-84e0-20225150a456","execution":{"iopub.status.busy":"2024-10-08T11:30:27.635881Z","iopub.execute_input":"2024-10-08T11:30:27.636889Z","iopub.status.idle":"2024-10-08T11:30:33.901104Z","shell.execute_reply.started":"2024-10-08T11:30:27.636844Z","shell.execute_reply":"2024-10-08T11:30:33.900238Z"},"trusted":true,"colab":{"referenced_widgets":["1daf8659cc5a4a878a6a408310c8619b","7973edb3f61c48feb5215850ce48e4fa","99884d7f51dc40ac96a17830f0842c5a","04bad41d94e040498df1c2d59d515b77","6dade557699e4642897159a0d4815da3"]}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.07k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1daf8659cc5a4a878a6a408310c8619b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7973edb3f61c48feb5215850ce48e4fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99884d7f51dc40ac96a17830f0842c5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04bad41d94e040498df1c2d59d515b77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6dade557699e4642897159a0d4815da3"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 120000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 7600\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":["`load_dataset`은 HuggingFace의 `datasets` library의 함수로, HuggingFace의 hub에서 dataset을 다운로드 받을 수 있도록 만든 함수입니다.\n","출력 결과를 보시면 `ag_news`는 `train`, `test` data로 구성되어있습니다.\n","이 중에서 우리는 `train`과 `test`를 활용합니다.\n","\n","`train` data를 한 번 살펴보겠습니다."],"metadata":{"id":"BT1VSMq-iuRl"}},{"cell_type":"code","source":["ag_news['train'][0]"],"metadata":{"id":"9nVFOdAIi4Hj","executionInfo":{"status":"ok","timestamp":1728273738795,"user_tz":-540,"elapsed":336,"user":{"displayName":"조성남","userId":"10831514774218419222"}},"outputId":"0736426e-6a3a-4b62-8f67-a14a2335e5d2","execution":{"iopub.status.busy":"2024-10-08T11:30:33.903705Z","iopub.execute_input":"2024-10-08T11:30:33.90437Z","iopub.status.idle":"2024-10-08T11:30:33.912291Z","shell.execute_reply.started":"2024-10-08T11:30:33.904324Z","shell.execute_reply":"2024-10-08T11:30:33.911417Z"},"trusted":true},"execution_count":null,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\",\n 'label': 2}"},"metadata":{}}]},{"cell_type":"markdown","source":["이번에는 tokenizer를 불러와서 미리 text들을 tokenize합니다."],"metadata":{"id":"6YdYEPD6nXzx"}},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")#문맥을 파악하는 데이터셋\n","\n","def preprocess_function(data):\n","    return tokenizer(data[\"text\"], truncation=True)\n","\n","ag_news_tokenized = ag_news.map(preprocess_function, batched=True)"],"metadata":{"id":"BhC_ETFZZRJ2","executionInfo":{"status":"ok","timestamp":1728273938775,"user_tz":-540,"elapsed":4488,"user":{"displayName":"조성남","userId":"10831514774218419222"}},"outputId":"7c3520ba-8f8b-4368-ac7d-2cfb4ba64da3","execution":{"iopub.status.busy":"2024-10-08T11:30:33.913431Z","iopub.execute_input":"2024-10-08T11:30:33.914214Z","iopub.status.idle":"2024-10-08T11:30:51.418597Z","shell.execute_reply.started":"2024-10-08T11:30:33.914164Z","shell.execute_reply":"2024-10-08T11:30:51.417652Z"},"trusted":true,"colab":{"referenced_widgets":["5a5ba976c7694b55a48d6103151d489e","8d94c83b7de0489eaa52966b4cbef6db","7307035a6375467aab70e173e3b43ee8","a705adbc9987493aae9125ba284bfed6","a8e9baaa7bbf4d769b25d96f2491dc17","521033d922ea46c6b4380618a9e5b36f"]}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a5ba976c7694b55a48d6103151d489e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d94c83b7de0489eaa52966b4cbef6db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7307035a6375467aab70e173e3b43ee8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a705adbc9987493aae9125ba284bfed6"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8e9baaa7bbf4d769b25d96f2491dc17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"521033d922ea46c6b4380618a9e5b36f"}},"metadata":{}}]},{"cell_type":"markdown","source":["Tokenizer를 실행할 때 넘겨주었던 `truncation` 옵션은 주어진 text가 일정 길이 이상이면 잘라내라는 의미입니다.\n","만약 특정 길이 값이 같이 주어지지 않는다면 `bert-base-cased`를 학습할 때 사용한 text의 최대 길이를 기준으로 값을 결정합니다."],"metadata":{"id":"ba2jcRcLpjc1"}},{"cell_type":"code","source":["ag_news_tokenized['train'][0].keys()"],"metadata":{"id":"i1aK6gpPqSSU","executionInfo":{"status":"ok","timestamp":1728273943889,"user_tz":-540,"elapsed":297,"user":{"displayName":"조성남","userId":"10831514774218419222"}},"outputId":"532ef65b-57fd-4b83-e20c-5c85273eb559","execution":{"iopub.status.busy":"2024-10-08T11:30:51.419897Z","iopub.execute_input":"2024-10-08T11:30:51.42032Z","iopub.status.idle":"2024-10-08T11:30:51.43032Z","shell.execute_reply.started":"2024-10-08T11:30:51.420263Z","shell.execute_reply":"2024-10-08T11:30:51.429444Z"},"trusted":true},"execution_count":null,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"dict_keys(['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'])"},"metadata":{}}]},{"cell_type":"markdown","source":["마지막 출력 결과를 보면, `text`와 `label` 이외에 `input_ids`가 생기신 것을 확인하실 수 있습니다.\n","이는 우리가 `AutoTokenizer.from_pretrained`로 불러온 tokenizer로 text를 token들로 나누고 정수 index로 변환한 결과입니다.\n","\n","이번에는 `train` data를 쪼개 training data와 validation data를 만들어보겠습니다."],"metadata":{"id":"Wfq0Ih4hqXYC"}},{"cell_type":"code","source":["ag_news_split = ag_news_tokenized['train'].train_test_split(test_size=0.2)\n","ag_news_train, ag_news_val = ag_news_split['train'], ag_news_split['test']\n","ag_news_test = ag_news_tokenized['test']"],"metadata":{"id":"RIXTFBTobI8N","execution":{"iopub.status.busy":"2024-10-08T11:30:51.431442Z","iopub.execute_input":"2024-10-08T11:30:51.431739Z","iopub.status.idle":"2024-10-08T11:30:51.723101Z","shell.execute_reply.started":"2024-10-08T11:30:51.431707Z","shell.execute_reply":"2024-10-08T11:30:51.722354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["HuggingFace `datasets`로 불러온 dataset은 `train_test_split`으로 쉽게 쪼갤 수 있습니다.\n","\n","다음은 각 split의 크기입니다."],"metadata":{"id":"BS1FltWfqjnz"}},{"cell_type":"code","source":["len(ag_news_train), len(ag_news_val), len(ag_news_test)"],"metadata":{"id":"5SGfOwadaYlk","executionInfo":{"status":"ok","timestamp":1728274193426,"user_tz":-540,"elapsed":319,"user":{"displayName":"조성남","userId":"10831514774218419222"}},"outputId":"9e08f3a7-91b2-4e62-bf33-b9f4c11b7518","execution":{"iopub.status.busy":"2024-10-08T11:30:51.724217Z","iopub.execute_input":"2024-10-08T11:30:51.72451Z","iopub.status.idle":"2024-10-08T11:30:51.731881Z","shell.execute_reply.started":"2024-10-08T11:30:51.724479Z","shell.execute_reply":"2024-10-08T11:30:51.730998Z"},"trusted":true},"execution_count":null,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(96000, 24000, 7600)"},"metadata":{}}]},{"cell_type":"markdown","source":["## Model 구현\n","\n","이번에는 text 분류를 수행할 Transformer를 구현합니다.\n","이전에는 Transformer의 구성 요소들을 직접 구현하여 합쳤습니다.\n","이번에는 HuggingFace의 BERT를 활용하여 인자만 넘겨주는 식으로 구현해보겠습니다:"],"metadata":{"id":"cGe7Fa9wqrCC"}},{"cell_type":"code","source":["from transformers import BertConfig\n","\n","config = BertConfig()\n","\n","config.hidden_size = 64  # BERT layer의 기본 hidden dimension\n","config.intermediate_size = 64  # FFN layer의 중간 hidden dimension\n","config.num_hidden_layers = 2  # BERT layer의 개수\n","config.num_attention_heads = 4  # Multi-head attention에서 사용하는 head 개수\n","config.num_labels = 4  # 마지막에 예측해야 하는 분류 문제의 class 개수\n","\n","model = AutoModelForSequenceClassification.from_config(config)"],"metadata":{"id":"xw-ZdYVCZfxU","execution":{"iopub.status.busy":"2024-10-08T11:30:51.733077Z","iopub.execute_input":"2024-10-08T11:30:51.733423Z","iopub.status.idle":"2024-10-08T11:30:51.932603Z","shell.execute_reply.started":"2024-10-08T11:30:51.733391Z","shell.execute_reply":"2024-10-08T11:30:51.931831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["BERT는 이전에 배운 Transformer의 architecture를 그대로 사용합니다.\n","그래서 BERT의 옵션들만 수정하면 vanilla Transformer를 쉽게 구현할 수 있습니다.\n","\n","Transformer 구현 이외에 분류 문제에 맞춰 첫 번째 token을 linear classifier를 거치는 등의 과정은 `AutoModelForSequenceClassification`이 구현해줍니다.\n","즉, 우리가 `config`로 넘겨주는 BERT의 마지막에 linear classifier를 달아주는 역할을 합니다."],"metadata":{"id":"BQpVs-lirLp5"}},{"cell_type":"markdown","source":["## 학습 코드\n","\n","다음은 위에서 구현한 Transformer를 ag_news로 학습하는 코드를 구현합니다.\n","먼저 다음과 같이 학습 인자들을 정의합니다."],"metadata":{"id":"31PYtx_Vrj1Y"}},{"cell_type":"code","source":["from transformers import TrainingArguments, Trainer\n","\n","training_args = TrainingArguments(\n","    output_dir='hf_transformer_ag_news',  # 모델, log 등을 저장할 directory\n","    num_train_epochs=10,  # epoch 수\n","    per_device_train_batch_size=128,  # training data의 batch size\n","    per_device_eval_batch_size=128,  # validation data의 batch size\n","    logging_strategy=\"epoch\",  # Epoch가 끝날 때마다 training loss 등을 log하라는 의미\n","    do_train=True,  # 학습을 진행하겠다는 의미\n","    do_eval=True,  # 학습 중간에 validation data에 대한 평가를 수행하겠다는 의미\n","    eval_strategy=\"epoch\",  # 매 epoch가 끝날 때마다 validation data에 대한 평가를 수행한다는 의미\n","    save_strategy=\"epoch\",  # 매 epoch가 끝날 때마다 모델을 저장하겠다는 의미\n","    learning_rate=1e-3,  # optimizer에 사용할 learning rate\n","    load_best_model_at_end=True  # 학습이 끝난 후, validation data에 대한 성능이 가장 좋은 모델을 채택하겠다는 의미\n",")"],"metadata":{"id":"YtqLwO7sZhXT","execution":{"iopub.status.busy":"2024-10-08T11:30:51.935433Z","iopub.execute_input":"2024-10-08T11:30:51.93574Z","iopub.status.idle":"2024-10-08T11:30:53.397107Z","shell.execute_reply.started":"2024-10-08T11:30:51.935708Z","shell.execute_reply":"2024-10-08T11:30:53.396351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["각각의 부분들은 이전 주차에서 배웠던 내용들을 설정하는 것에 불과하다는 것을 알 수 있습니다.\n","요약하면 다음과 같습니다:\n","- `epochs`: training data를 몇 번 반복할 것인지 결정합니다.\n","- `batch_size`: training data를 얼마나 잘게 잘라서 학습할 것인지 결정합니다.\n","- `learning_rate`: optimizer의 learning rate를 얼마로 할 것인지 결정합니다.\n","위의 부분들 이외에도 사소한 구현 요소들도 지정할 수 있습니다.\n","\n","다음은 loss 이외의 평가 함수들을 구현하는 방법입니다."],"metadata":{"id":"rKvB31IlsK2f"}},{"cell_type":"code","source":["import evaluate\n","\n","accuracy = evaluate.load(\"accuracy\")\n","\n","\n","def compute_metrics(pred):\n","    predictions, labels = pred\n","    predictions = np.argmax(predictions, axis=-1)#배열의 마지막 축을 따라 계산\n","    return accuracy.compute(predictions=predictions, references=labels)"],"metadata":{"id":"Wa2zYLTUZ8rD","executionInfo":{"status":"ok","timestamp":1725798823141,"user_tz":-540,"elapsed":722,"user":{"displayName":"조승혁","userId":"15759752471844115325"}},"outputId":"843aba34-6f18-42e5-d732-6e2ff658a69e","execution":{"iopub.status.busy":"2024-10-08T11:30:53.398087Z","iopub.execute_input":"2024-10-08T11:30:53.398412Z","iopub.status.idle":"2024-10-08T11:30:54.727203Z","shell.execute_reply.started":"2024-10-08T11:30:53.39838Z","shell.execute_reply":"2024-10-08T11:30:54.726359Z"},"trusted":true,"colab":{"referenced_widgets":["d95095967cac45bbaf246a9ee761b1b2"]}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d95095967cac45bbaf246a9ee761b1b2"}},"metadata":{}}]},{"cell_type":"markdown","source":["`evaluate` 또한 HuggingFace의 library로 다양한 평가 함수들을 제공하고 있습니다.\n","이번 실습의 경우, 감정 분석 문제는 분류 문제이기 때문에 정확도를 계산할 수 있습니다.\n","위와 같이 예측 결과(`pred`)와 실제 label(`labels`)가 주어졌을 때 정확도를 계산하는 것은 `evaluate`의 accuracy 함수로 구현할 수 있습니다.\n","\n","마지막으로 위의 요소들을 종합하여 학습할 수 있는 `Trainer`를 구현합니다."],"metadata":{"id":"tGENAMKk-4I2"}},{"cell_type":"code","source":["from transformers import EarlyStoppingCallback\n","\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=ag_news_train,\n","    eval_dataset=ag_news_val,\n","    compute_metrics=compute_metrics,\n","    tokenizer=tokenizer,\n","    # callbacks = [EarlyStoppingCallback(early_stopping_patience=1)]\n",")"],"metadata":{"id":"Ci4lNfK6Z_z4","execution":{"iopub.status.busy":"2024-10-08T11:30:54.728492Z","iopub.execute_input":"2024-10-08T11:30:54.728882Z","iopub.status.idle":"2024-10-08T11:30:55.645891Z","shell.execute_reply.started":"2024-10-08T11:30:54.728838Z","shell.execute_reply":"2024-10-08T11:30:55.644922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["모델, training 인자, training과 validation data, 부가적인 평가 함수, 그리고 tokenizer를 넘겨주면 끝입니다.\n","별개로 early stopping과 같은 기능도 주석 친 부분과 같이 `callbacks`로 구현할 수 있으니 참고해주시길 바랍니다.\n","\n","위와 같이 만든 `Trainer`는 다음과 같이 학습을 할 수 있습니다."],"metadata":{"id":"MOK4poi1_Mgh"}},{"cell_type":"code","source":["import torch.amp\n","\n","trainer.train()"],"metadata":{"id":"qmNkWuVVaBEc","executionInfo":{"status":"ok","timestamp":1725797895052,"user_tz":-540,"elapsed":395226,"user":{"displayName":"조승혁","userId":"15759752471844115325"}},"outputId":"7a610572-458b-4577-f838-3d2488268616","execution":{"iopub.status.busy":"2024-10-08T11:30:55.64715Z","iopub.execute_input":"2024-10-08T11:30:55.647521Z","iopub.status.idle":"2024-10-08T11:43:09.817729Z","shell.execute_reply.started":"2024-10-08T11:30:55.647478Z","shell.execute_reply":"2024-10-08T11:43:09.81685Z"},"trusted":true,"colab":{"referenced_widgets":["d7b4e16b20144d9aa6378cb612b89e88"]}},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113567311111992, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7b4e16b20144d9aa6378cb612b89e88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241008_113243-56b923vu</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/csn2506-diem/huggingface/runs/56b923vu' target=\"_blank\">hf_transformer_ag_news</a></strong> to <a href='https://wandb.ai/csn2506-diem/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/csn2506-diem/huggingface' target=\"_blank\">https://wandb.ai/csn2506-diem/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/csn2506-diem/huggingface/runs/56b923vu' target=\"_blank\">https://wandb.ai/csn2506-diem/huggingface/runs/56b923vu</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7500/7500 10:21, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.480100</td>\n      <td>0.294155</td>\n      <td>0.906542</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.223900</td>\n      <td>0.252716</td>\n      <td>0.916083</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.156700</td>\n      <td>0.269440</td>\n      <td>0.917375</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.111300</td>\n      <td>0.302647</td>\n      <td>0.910917</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.078800</td>\n      <td>0.335344</td>\n      <td>0.911042</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.054500</td>\n      <td>0.400669</td>\n      <td>0.907500</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.038100</td>\n      <td>0.430258</td>\n      <td>0.907583</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.026300</td>\n      <td>0.451207</td>\n      <td>0.910417</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.018100</td>\n      <td>0.504697</td>\n      <td>0.907375</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.013200</td>\n      <td>0.522610</td>\n      <td>0.907042</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=7500, training_loss=0.12010427207946778, metrics={'train_runtime': 733.134, 'train_samples_per_second': 1309.447, 'train_steps_per_second': 10.23, 'total_flos': 57181958676480.0, 'train_loss': 0.12010427207946778, 'epoch': 10.0})"},"metadata":{}}]},{"cell_type":"markdown","source":["보시다시피 training loss는 잘 떨어지는 반면, validation loss는 중간부터 쭉 올라가는 것을 볼 수 있습니다.\n","Overfitting이 일어났다고 볼 수 있습니다.\n","\n","위와 같이 학습이 끝난 후 validation loss가 가장 낮은 모델을 가지고 test data의 성능을 평가하는 것은 다음과 같이 구현할 수 있습니다."],"metadata":{"id":"IuGFTHER_a_U"}},{"cell_type":"markdown","source":["## 최종 모델의 test split에 대한 정확도가 print되어야 하며, 90%를 넘기셔야 합니다."],"metadata":{"id":"AIgyr63g9_p4"}},{"cell_type":"code","source":["eval_results = trainer.evaluate(ag_news_val)\n","\n","print(eval_results)\n","print(f'eval_accuracy = {eval_results[\"eval_accuracy\"]:.2%}')\n"],"metadata":{"id":"16mRdiEgeVPQ","executionInfo":{"status":"ok","timestamp":1725797932943,"user_tz":-540,"elapsed":25436,"user":{"displayName":"조승혁","userId":"15759752471844115325"}},"outputId":"50422d77-9688-47e2-d59b-e7e5a0305602","execution":{"iopub.status.busy":"2024-10-08T11:43:09.818806Z","iopub.execute_input":"2024-10-08T11:43:09.819074Z","iopub.status.idle":"2024-10-08T11:43:18.964175Z","shell.execute_reply.started":"2024-10-08T11:43:09.819044Z","shell.execute_reply":"2024-10-08T11:43:18.963201Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [188/188 00:08]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.2527160346508026, 'eval_accuracy': 0.9160833333333334, 'eval_runtime': 9.1321, 'eval_samples_per_second': 2628.082, 'eval_steps_per_second': 20.587, 'epoch': 10.0}\neval_accuracy = 91.61%\n","output_type":"stream"}]},{"cell_type":"markdown","source":["# 학습결과 : eval_accuracy = 91.61%"],"metadata":{"id":"h05cUK6N_te4"}},{"cell_type":"code","source":["trainer.save_model()"],"metadata":{"id":"kRVBuZ4bl4N1","execution":{"iopub.status.busy":"2024-10-08T11:43:18.965298Z","iopub.execute_input":"2024-10-08T11:43:18.965592Z","iopub.status.idle":"2024-10-08T11:43:19.008708Z","shell.execute_reply.started":"2024-10-08T11:43:18.965559Z","shell.execute_reply":"2024-10-08T11:43:19.007938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["그리고 저장한 모델을 가지고 다른 예시들을 예측하는 것은 다음과 같이 구현할 수 있습니다."],"metadata":{"id":"QUXpdaWu__v0"}},{"cell_type":"code","source":["from transformers import pipeline\n","\n","\n","classifier = pipeline(\"sentiment-analysis\", model=\"./hf_transformer_ag_news/\", device='cuda')\n","example_text = \"UK charges 8 in terror plot linked to alert in US LONDON, AUGUST 17: Britain charged eight terror suspects on Tuesday with conspiracy to commit murder and sai\"\n","result = classifier(example_text)\n","print(f'score = {result[0][\"score\"]:.2%}')"],"metadata":{"id":"ydZg3R7FdwKq","executionInfo":{"status":"ok","timestamp":1725798314586,"user_tz":-540,"elapsed":338,"user":{"displayName":"조승혁","userId":"15759752471844115325"}},"outputId":"26092c72-41e4-4f24-8262-80e0457852f6","execution":{"iopub.status.busy":"2024-10-08T11:49:22.760349Z","iopub.execute_input":"2024-10-08T11:49:22.760743Z","iopub.status.idle":"2024-10-08T11:49:22.841695Z","shell.execute_reply.started":"2024-10-08T11:49:22.760711Z","shell.execute_reply":"2024-10-08T11:49:22.840754Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"score = 99.47%\n","output_type":"stream"}]},{"cell_type":"markdown","source":["# score = 99.47%"],"metadata":{"id":"PvNP33679_p5"}},{"cell_type":"markdown","source":["HuggingFace의 `pipeline`은 다양한 모델들에 대하여 서비스에 사용할 수 있는 형태들을 제공합니다.\n","여기서는 영화 리뷰가 주어졌을 때, 그 신뢰도를 `score`로 넘겨주게 됩니다.\n","\n","이처럼 HuggingFace를 활용하면 모델이나 예측, 학습 코드를 구현할 필요 없이 인자로 설정값들만 넘겨주면 쉽게 구현 할 수 있습니다."],"metadata":{"id":"U_uPElGAAGwH"}}]}